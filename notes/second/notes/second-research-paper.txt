START
Теперь самое время рассказать о задаче, которую мы поставили перед собой в этой исследовательской работе! 
Наша команда разрабатывает виртуальный музей на базе Cyber-Physical-Social Systems (CPSS) — это такая сложная система, которая объединяет физический мир, цифровую среду и социальные взаимодействия в единое целое. Представьте себе: настоящий робот TurtleBot2 бродит по реальной лаборатории, пользователи в VR-очках исследуют виртуальные экспонаты кипрского культурного наследия, а система в реальном времени генерирует контекстную информацию об экспонатах. Звучит как научная фантастика, но это реальность!
Проблема заключалась в том, что LLM в системе периодически "галлюцинировала" — выдавала фактически неточную или просто выдуманную информацию о культурных артефактах. Когда речь идет о культурном наследии, точность критически важна. Никому не хочется, чтобы виртуальный гид рассказывал туристам небылицы о древних кипрских храмах!
Поэтому мы решили сравнить два подхода: базовую предобученную модель Mistral 7B против нашей кастомной версии, fine-tuned на специфичных данных о кипрском культурном наследии с помощью LoRA. Задача была понять, насколько fine-tuning улучшает качество ответов по пяти критериям: фактическая точность, релевантность контексту, безопасность, соответствие человеческим ожиданиям и правильность форматирования ответов.

INITIAL
Этот блог будет Посвящен моей второй исследовательской работе, которая в свою очередь является продлжением первой, но с углубленной работой, я познакомлю вас с тем как целью было тренировка выбранной модели при помощи qlora вместо использования rag approach, с последующим human feedback, и что из этого вышло.
Продолжаю свою мысль с точки завершения первой исследовательской работы, с учетом того что она была принята и мы все были рады проделанной работой, это дало нам зеленый свет на продолжение наших идей, которые мы spoiler, также успешно реализовали и работа были приняты.
Все начинается с чего? Правильно, тестирование. Нашей задачей состояло в первую очередь работа с SLM моделями, модели маленького размера, для последующего удоюного деплоя в проде и с минимальным количеством обязанностей (например такова модель могла бы быть подключена в virtual reality и быть ассистентом не привязанным к rag, но при этом могла вести какой то осмысленный диалог с нужным набором знаний).
Мне нравится сфера машинного обучения тем, что это полностью прикладное направление, для того чтобы сделать качественную, работающую модель, тебя ждет череда сломанных моделей, часов попыток, в процессе которого ты учишься, какие оптимизаторы лучше использовать, как можно ускорить модель, почему lr влияет на обучение, и всему этому я научился во время постройки Lora pipeline.
На самом деле, стоит уточнить что в работе мы использовали подход qlora, а не lora, мы использовали квантизацию bits and bytes для уменьшения использованны мощностей, а с учетом того что у нас были slm, это не сыграло большой разницы, да и сам bits and bytes зачастую дает больше пллюсов чем минусов.

LORA ARCHITECTURE
Давайте разберем архитектуру этого LoRA pipeline более детально! 
Наш код состоит из двух основных классов, которые работают как конвейер. Первый класс `LLMLoaderPipeline` — это наш подготовительный цех. Он загружает модель с квантизацией (помните, мы используем QLoRA, поэтому BitsAndBytesConfig сжимает веса с 16 бит до 8), читает текстовые данные из файла, разбивает их на абзацы и токенизирует. Особенно мне нравится, как он автоматически устанавливает `pad_token = eos_token` — мелочь, но спасает от головной боли с кастомными моделями, у которых может не быть pad токена по умолчанию.
Второй класс `LoraTrainerPipeline` — это сердце всей операции. Здесь происходит настоящая магия: мы берем базовую модель, подготавливаем её для k-bit тренировки, накладываем LoRA адаптеры с рангом 8 (компромисс между скоростью и точностью), и запускаем обучение. Конфигурация LoRA нацелена на модули `q_proj` и `v_proj` — это query и value проекции в attention механизме, именно те части, где LoRA показывает наилучшие результаты. Альфа параметр в 42 — это влияние наших LoRA весов на исходную модель, а дропаут 0.05 помогает избежать переобучения.
Весь pipeline завершается методом `merge_and_unload()`, который объединяет LoRA адаптеры с базовой моделью в единое целое. Это значит, что на выходе мы получаем полноценную модель, а не базовую модель плюс отдельные адаптеры.
После того как мы перепробовали разные модели на тестировании (tinyllama, mistral, phi, llama подошли к нашей задаче больше всего) (tinyllama был как всегда нашим испытуемым на котором тестировался pipeline, благодаря ее маленькой количеству параметров).

'''
Attaching image of LORA ARCHITECTURE
'''

END
Были конечно попытки с моей стороны написать также кастомный RAG Pipeline при помощи cosine simmilarities, но от этой идеи решили отказаться так как время поджимало, на всю работу дали меньше месяца, за всю работу отвечал ваш слуга Никита :). Наверное вы уже устали слышать то, что моя работа была также опубликована и что в который раз мне повезло, и нашу работа была принята.
'''
Напиши что то красочное в конце
'''